{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"img\"\n",
    "men_folder=os.listdir(os.path.join(root_dir,\"MEN\"))\n",
    "women_folder=os.listdir(os.path.join(root_dir,\"WOMEN\"))\n",
    "men_folder=[os.path.join(\"MEN\",i) for i in men_folder]\n",
    "women_folder=[os.path.join(\"WOMEN\",i) for i in women_folder]\n",
    "classes=men_folder+women_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBaseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 256)  \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNBaseNetwork().to(device)\n",
    "state=torch.load(\"trained_models/cloth_retrieval_10.pth\",map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state[\"base_net\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "database=[]\n",
    "labels=[]\n",
    "for style in classes:\n",
    "    for imgid in os.listdir(os.path.join(root_dir,style)):\n",
    "        for i in os.listdir(os.path.join(root_dir,style,imgid)):\n",
    "            labels.append(os.path.join(root_dir,style,imgid,i))\n",
    "            img = Image.open(os.path.join(root_dir,style,imgid,i)).convert('RGB')\n",
    "            img=img.point(lambda p: p / 255.0)\n",
    "            img=img.resize((256,256))\n",
    "            transform = transforms.ToTensor()\n",
    "            image_tensor = transform(img).to(device)\n",
    "            emb=model(torch.unsqueeze(image_tensor, 0)).detach().cpu()\n",
    "            emb=torch.squeeze(emb,0)\n",
    "            database.append(emb)\n",
    "        \n",
    "    print(style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image_embedding(path,model,device):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img=img.point(lambda p: p / 255.0)\n",
    "    img=img.resize((256,256))\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensor = transform(img).to(device)\n",
    "    img_embed=model(torch.unsqueeze(image_tensor, 0)).detach().cpu()\n",
    "    return img_embed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_database = [tensor.flatten().numpy() for tensor in database]\n",
    "\n",
    "X = np.array(flattened_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labels, f)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc628f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "n_neighbors = 5  # Number of neighbors to consider\n",
    "knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "knn.fit(X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving most similar garments from database for a test input\n",
    "test_image_path=\"test_image.jpg\"\n",
    "query_embedding=test_image_embedding(test_image_path,model,device)\n",
    "distances, indices = knn.kneighbors(query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d846265",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(test_image_path).convert('RGB')\n",
    "plt.imshow(test_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_neighbors):\n",
    "    similar_shop_image_path=labels[indices[0][i]]\n",
    "    img = Image.open(similar_shop_image_path).convert('RGB')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get only one output image for a given test input image we can make n_neighbors = 1 before fitting knn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
