{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs():\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    root_dir=\"img\"\n",
    "    men_folder=os.listdir(os.path.join(root_dir,\"MEN\"))\n",
    "    women_folder=os.listdir(os.path.join(root_dir,\"WOMEN\"))\n",
    "    men_folder=[os.path.join(\"MEN\",i) for i in men_folder]\n",
    "    women_folder=[os.path.join(\"WOMEN\",i) for i in women_folder]\n",
    "    classes=men_folder+women_folder\n",
    "    for style in classes:\n",
    "        style_folder = os.path.join(root_dir,style)\n",
    "        img_files = os.listdir(style_folder)\n",
    "        \n",
    "        for i in range(len(img_files)-1):\n",
    "            img1=os.listdir(os.path.join(style_folder,img_files[i]))[0]\n",
    "            img1=os.path.join(style_folder,img_files[i],img1)\n",
    "            num_elements_to_select = min(17,len(img_files))\n",
    "\n",
    "            random_elements = random.sample(img_files, num_elements_to_select)\n",
    "            for j in range(len(random_elements)):\n",
    "                img2=os.listdir(os.path.join(style_folder,random_elements[j]))[0]\n",
    "                img2=os.path.join(style_folder,random_elements[j],img2)\n",
    "                pairs.append([img1,img2])\n",
    "                labels.append(1)\n",
    "                \n",
    "            neg_style=[i for i in classes if i!=style]\n",
    "            for k in neg_style:\n",
    "                neg_style_folder=os.path.join(root_dir,k)\n",
    "                neg_img_files=os.listdir(neg_style_folder)\n",
    "                random_neg_img_file=random.choice(neg_img_files)\n",
    "                img2=os.listdir(os.path.join(neg_style_folder,random_neg_img_file))[0]\n",
    "                img2=os.path.join(neg_style_folder,random_neg_img_file,img2)\n",
    "                pairs.append([img1,img2])\n",
    "                labels.append(0)\n",
    "                    \n",
    "        print(style)\n",
    "    return np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pairs, tr_y = create_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f169626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_paths,labels, transform):\n",
    "        self.pair_paths = pair_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        img1_path, img2_path = self.pair_paths[index]\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "        img1 = self.transform(img1)\n",
    "        img2 = self.transform(img2)\n",
    "        return img1/255.0, img2/255.0 ,label\n",
    "\n",
    "custom_dataset = CustomDataset(tr_pairs,tr_y, transform=preprocess)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, euclidean_distance, target):\n",
    "        loss = torch.mean(target * torch.pow(euclidean_distance, 2) +\n",
    "                          (1 - target) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca691f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBaseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 256)  \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, base_network):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.base_network = base_network\n",
    "    \n",
    "    def forward(self, input_a, input_b):\n",
    "        processed_a = self.base_network(input_a)\n",
    "        processed_b = self.base_network(input_b)\n",
    "        distance = torch.norm(processed_a - processed_b, dim=1, keepdim=True)\n",
    "        return distance\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "base_network = CNNBaseNetwork()\n",
    "\n",
    "\n",
    "siamese_network = SiameseNetwork(base_network)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "siamese_network.to(device)\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "\n",
    "optimizer = optim.RMSprop(siamese_network.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90af716",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"base_net\":base_network.state_dict(),\"opt\":optimizer.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for img1,img2, labels in tqdm(data_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "        img1,img2,labels=img1.to(device),img2.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_network(img1, img2)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(state,f\"cloth_retrieval_{epoch}.pth\")   \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(data_loader)}')\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a5c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae51e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2524c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95dd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b3886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd08d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba71a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
